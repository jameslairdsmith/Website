---
title: "Scrantonicity - Part 2"
author: "EE"
date: "2020-04-13"
description: "Clustering models using The Office dialogue"
output: html_document
---

## Would I rather be feared or loved? Easy. Both.

A few weeks ago, I did some [exploratory analyses](https://eric-ekholm.netlify.com/blog/office-part1/) of dialogue from The Office. That blog could easily have been a lot longer than it was, and so instead of writing some gigantic post that would have taken 30 minutes+ to read, I decided to separate it out into several different blog posts. And so here's volume 2.

In this post, I want to try using a couple of different clustering algorithms to identify patterns in dialogue from the show. First, I'm going to use k-means clustering to identify patterns in who talks to whom in different episodes. Second, I'm going to use topic modeling, and latent Dirichlet allocation (LDA) more specifically, to identify common topics across episodes.

Once again, huge thanks to Brad Lindblad, the creator of the `{schrute}` package for R, which makes the dialogue from The Office easy to work with. Also, the LDA section of this post is strongly informed by Julia Silge and David Robinson's [Text Mining with R](https://www.tidytextmining.com/) book, which is definitely worth a read.

### Setup

As in the previous blog, I'll be using the `{schrute}` package to get the transcripts from the show, and I'm going to limit the dialogue to the first 7 seasons of the show, which is when Michael Scott was around. I'll also use a handful of other packages for data cleaning, analysis, and visualization. Let's load all of this in and do some general setup.

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE)

library(schrute) #office dialogue
library(tidyverse) #data wrangling tools
library(broom) #tidying models
library(tidytext) #tools for working with text data
library(knitr) #markdown functionality
library(kableExtra) #styling for tables
library(hrbrthemes) #ggplot themes
library(topicmodels) #topic modeling functions

theme_set(theme_ipsum())

purple <- "#3f2d54" #getting the purple color from the ipsum palette

office <- theoffice %>%
  filter(as.numeric(season) <= 7)
```

## I'm not superstitious, but I am a little stitious.

Now that we have our data read in and our packages loaded, let's start with the cluster analysis. The goal here is going to be to figure out if there are certain "types" (clusters, groups, whatever you want to call them) of episodes. There are several frameworks we could use to go about doing this. One approach would be a mixture modeling approach (e.g. latent profile analysis, latent class analysis). I'm not doing that here because I want each episode to be an observation when we cluster, and I'm not sure we have enough episodes here to get good model fits using this approach. Instead, I'm going to use k-means clustering, which basically places observations (episodes, in this case) into one of _k_ groups (where _k_ is supplied by the user) by trying to minimize the "distance" between each observation and the center of the group. The algorithm iteratively assigns observations to groups, updates the center of each group, reassigns observations to groups, etc. until it reaches a stable solution.

We can also include all sorts of different variables in the k-means algorithm to serve as indicators. For this analysis, I'm going to use the number of exchanges between different characters per episode -- i.e. the number of exchanges between Michael and Jim, between Jim and Dwight, etc. -- to estimate groups. This could tell us, for instance, that one "type" of Office episode features lots of exchanges between Michael and Dwight, lots between Pam and Jim, and few between Pam and Michael. One consideration when we use the k-means algorithm is that, because we're looking at distance between observations, we typically want our observations to be on the same scale. Fortunately, since all of our indicators will be "number of lines per episode," they're already on the same scale, so we don't need to worry about standardizing.

Let's go ahead and set up our data. I'm also going to decide to only use the 5 characters who speak the most during the first 7 seasons in this analysis, otherwise the number of combinations of possible exchanges would be huge. These five characters are:

```{r top chars}
top5_chars <- office %>%
  count(character, sort = TRUE) %>%
  top_n(5) %>%
  pull(character)

top5_chars
```

Ok, so our top 5 characters here are Michael, Dwight, Jim, Pam, and Andy. Since Andy doesn't join the show until season 3, I'm actually going to narrow our window of usable episodes to those in seasons 3-7. Otherwise, the clustering algorithm would likely group episodes with a focus on those in seasons 1 and 2, where Andy will obviously have 0 lines, vs episodes in later seasons.

Additionally, we want to code our changes so that Michael & Jim is the same as Jim & Michael.
```{r cluster init}
combos <- t(combn(top5_chars, 2)) %>%
  as_tibble() %>%
  mutate(comb = glue::glue("{V1}&{V2}"),
         comb_inv = glue::glue("{V2}&{V1}"))

replace_comb <- combos$comb
names(replace_comb) <- combos$comb_inv


office_exchanges <- office %>%
  filter(as.numeric(season) >= 3) %>%
  mutate(char2 = lead(character)) %>% #this will tell us who the speaker is talking to
  filter(character %in% top5_chars &
         char2 %in% top5_chars &
         character != char2) %>% #this filters down to just exchanges between our top 5 characters
  mutate(exchange = glue::glue("{character}&{char2}") %>%
           str_replace_all(replace_comb)) %>%
  select(season, episode_name, character, char2, exchange) %>%
  count(season, episode_name, exchange) %>%
  pivot_wider(names_from = exchange,
              values_from = n,
              values_fill = list(n = 0))

head(office_exchanges)
```

Great -- now our data is all set up so that we know the number of lines exchanged between main characters in each episode. We can run some clustering algorithms now to see if there are patterns in these exchanges. To do this, we'll fit models testing out 1-10 clusters. We'll then look at the error for each of these models graphically and use this to choose how many clusters we want to include in our final model.

```{r cluster}
clusters_fit <- tibble(
  k = c(1:10),
  km_fit = map(c(1:10), ~kmeans(office_exchanges %>% select(-c(1:2)), centers = .))
) %>%
  mutate(within_ss = map_dbl(km_fit, ~pluck(., 5)))

clusters_fit %>%
  ggplot(aes(x = k, y = within_ss)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Within Cluster Sum of Squares vs K"
  )
```

We can see that error decreases as we add more clusters, and error will *always* decrease as k increases. But we can also see that the rate of decrease slows down a bit as we increase our number of clusters. Ideally, there would be a definitive bend, or "elbow" in this plot where the rate of decrease levels off (which is also the number of clusters we'd choose), but that's not quite the case here. It seems like there's some slight elbow-ing at 6 clusters, so let's just go ahead and choose that.

```{r}
office_clustered <- augment(clusters_fit$km_fit[[6]], data = office_exchanges)

clusters_long <- office_clustered %>%
  mutate(season = as_factor(season)) %>%
  group_by(.cluster) %>%
  summarize_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup() %>%
  pivot_longer(cols = -c(".cluster"),
               names_to = "chars",
               values_to = "lines")

clusters_long %>%
  ggplot(aes(x = lines, y = chars, fill = .cluster)) +
    geom_col() +
    facet_wrap(~.cluster, ncol = 2, scales = "free_y") +
    #scale_y_reordered() +
    scale_fill_ipsum() +
    theme_minimal() +
    labs(
      title = "Types of Office Episodes"
    ) +
    theme(
      legend.position = "none"
    )
  
```

So, these plots show us the average number of exchanges between characters by cluster. Cluster 1 shows relatively few exchanges between all of our main characters -- this probably means that there's a lot of side character action going on (recall that we didn't include exchanges between anyone other than Michael, Dwight, Jim, Pam, and Andy in our clustering algorithm). Cluster 2 are episodes where we have a ton of Michael & Dwight exchanges. Cluster 3 is more balanced but dominated by Michael & Andy exchanges. Cluster 4 might be called "Jim-centric" episodes -- we see lots of exchanges between Michael and Jim as well as between Dwight and Jim. Cluster 5 might be "Pam-centric" episodes, since there's a lot of Michael & Pam as well as Jim and Pam. And Cluster 6 looks like a more balanced version of Cluster 2 -- mostly Michael and Dwight, but other characters as well.

One thing to remember is that these clusters aren't necessarily balanced. As the table below shows, *most* episodes fit into Cluster 1, and only a handful fit into clusters 2, 3, and 4.

```{r}
office_clustered %>%
  count(.cluster, name = "num_episodes") %>%
  kable(format = "html") %>%
  kable_styling(bootstrap_options = c("condensed", "striped", "hover"))
```

## You're paying way too much for worms. Who's your worm guy?

The previous approach to clustering gave us a sense of who talks to whom, but what if we want to learn about the types of things characters in the show talk about? We can use topic modeling & LDA to examine the actual text from the transcripts and estimate common topics in the show.

First, let's set up our data. We need a slightly different structure than we used in the previous analysis. There's no reason to restrict to just seasons 3-7 from a modeling perspective, but for consistency with the previous section, I'm going to.

```{r}
office_words <- office %>%
  filter(season >= 3) %>%
  select(-text_w_direction) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  filter(str_detect(word, "\\d+", negate = TRUE)) %>%
  count(episode_name, word)

head(office_words)
```
Our data is now in a form where we have a count of the number of times each word is used per episode. To fit our LDA model, we'll need to turn this into a document-term matrix. Next, like we had to do with the k-means clustering, we need to figure out the appropriate number of topics to estimate. I'll use a similar approach here and fit models for several values of k (ranging from 2-10) and then choose final parameters from there.

```{r}
office_dtm <- office_words %>%
  cast_dtm(episode_name, word, n)


test <- LDA(office_dtm, k = 2, control = list(seed = 0408))

xx <- logLik(test) %>% as_vector()

office_lda_fits <- tibble(
  k = c(2:10),
  lda_fits = map(c(2:10), ~LDA(office_dtm, k = ., control = list(seed = 0408)))
) %>%
  mutate(ll = map_dbl(lda_fits, ~logLik(.)))
```


I'm less familiar with how to choose the optimal number of topics in an LDA model, but my general understanding is that we'll want to maximize the log likelihood of the model, so we'll take a look at that below. I'm sure there are more rigorous approaches, but we're going to stick with this one for now.
```{r}
office_lda_fits %>%
  ggplot(aes(x = k, y = ll)) +
  geom_point() +
  geom_line() +
  labs(
    title = "log likelihood of models with k topics"
  )
```

Great -- so let's go with 10 topics. It's possible that if we kept increasing the number of topics, the log likelihood would continue to increase here, but estimating a ton of topics take a lot of computation time and they likely become more prone to idiosyncratic differences between episodes. Plus I'm just doing this for funsies.

So, next, we want to try to get a sense of what these topics are. To do this, we want to look at the beta coefficients from the model, which represent the probability that a given word aligns with a given topic.

```{r}
lda_betas <- office_lda_fits$lda_fits[[9]] %>%
  tidy(matrix = "beta")

head(lda_betas)
```

```{r}
lda_betas %>%
  group_by(topic) %>%
  top_n(n = 10, wt = beta) %>%
  View()


lda_betas %>%
  filter(beta > .001) %>%
  pivot_wider(names_from = topic,
              values_from = beta,
              names_prefix = "topic_") %>%
  mutate_at(c(3:11), list(log = ~log(./topic_1))) %>%
  select(matches("term|log")) %>%
  pivot_longer(cols = matches("log"),
               names_to = "topic",
               values_to = "val") %>%
  mutate(topic = str_replace_all(topic, "(.*)_(\\d+)_(.*)", "\\2")) %>%
  group_by(topic) %>%
  top_n(n = 10, wt = abs(val)) %>%
  ungroup() %>%
  arrange(desc(topic)) %>%
  View()
```

I'm not sure the LDA thing is going to work.